{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b8e76ba",
   "metadata": {},
   "source": [
    "*description*\n",
    "\n",
    "script to\n",
    "- load\n",
    "- filter\n",
    "- correct\n",
    "- save\n",
    "harringtonine data as JSON file for HMM analysis.\n",
    "\n",
    "Filtering:\n",
    "- start in the first minute of acquisition;\n",
    "\n",
    "\n",
    "\n",
    "Add delay between HT and imaging to time directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3011b06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os.path import join\n",
    "\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "import re\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "### to be modified by the user ####\n",
    "input_dir = \"path_to_intensity_traces\"\n",
    "output_dir = \"path_to_results_directory\"\n",
    "####\n",
    "\n",
    "from FilterResults import smooth, remove_spikes\n",
    "\n",
    "from ExtractTraces import get_files_list, get_values, get_unique_IDs\n",
    "\n",
    "from date_cond2delay import date_cond2delay\n",
    "\n",
    "#-----------------#\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "factor = 0.3\n",
    "matplotlib.rcParams.update({\n",
    "        'figure.figsize' : (round(15),round(10)),\n",
    "        'axes.labelsize' : round(80*factor),\n",
    "        'axes.titlesize' : round(80*factor),\n",
    "        'xtick.labelsize' : round(80*factor),\n",
    "        'ytick.labelsize' : round(80*factor),\n",
    "        'lines.linewidth' : round(6*factor),\n",
    "        'lines.markersize' : round(34*factor),\n",
    "        'legend.fontsize' : round(60*factor),\n",
    "        'axes.linewidth' : round(10*factor),\n",
    "        'xtick.major.size': round(20*factor),\n",
    "        'ytick.major.size': round(20*factor),\n",
    "        'xtick.major.width': round(8*factor),\n",
    "        'ytick.major.width': round(8*factor),\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20394634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_translated(y, thr):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : 1d array\n",
    "        green fluorescent track.\n",
    "    thr : real\n",
    "        threshold discriminating translation from noise.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int: how many data points are above threshold.\n",
    "\n",
    "    \"\"\"\n",
    "    ysmoo = smooth(y, w=15, mean=False)\n",
    "    mask = ysmoo > thr\n",
    "    return len(ysmoo[mask])\n",
    "\n",
    "\n",
    "def estimate_n_translated(Y, thr):\n",
    "    n_translated = 0\n",
    "    for y in Y:\n",
    "        if is_translated(y, thr):\n",
    "            n_translated += 1\n",
    "    return n_translated\n",
    "\n",
    "def get_unique_IDs(files):\n",
    "    \n",
    "    unique_IDs = []\n",
    "    \n",
    "    for f in files:\n",
    "        \n",
    "        ID = re.search(r'track(\\d+)', f).group(1)\n",
    "        stage = re.search(r's(\\d)', f).group(1)\n",
    "        \n",
    "        unique_IDs.append(int(stage+ID))\n",
    "        \n",
    "    res = np.array(unique_IDs)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782a9cce",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d421dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ee3632",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c84acaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### modify by the user\n",
    "sample = \"sample_name\"\n",
    "dates = [\"date1\", \"date2\"] # list of experiment dates for sample\n",
    "#####\n",
    "\n",
    "model_name = \"gaussian_plane_v1\" #\"gaussian_plane\"\n",
    "spec = \"\"#\"_ff\"\n",
    "nframes = 271\n",
    "dt = 20 # sec\n",
    "\n",
    "time = np.arange(nframes) * 20 # time is seconds\n",
    "\n",
    "date2green = {} # dictionary containing date -> array of traces\n",
    "date2red = {} # dictionary containing date -> array of red traces\n",
    "date2IDs = {}\n",
    "\n",
    "date2gw = {} # green spot width\n",
    "date2rw = {} # red spot width\n",
    "\n",
    "_, axs = plt.subplots(2, figsize=(7,10), sharex=True)\n",
    "for i, date in enumerate(dates):\n",
    "\n",
    "    #-------- Load data ----------#\n",
    "    print(date)\n",
    "    files = get_files_list(date, sample, input_dir, model_name, spec, stage='*')\n",
    "    print(files)\n",
    "\n",
    "    new_files, green_traces, red_traces, mask = get_values(files, nframes)\n",
    "    trackIDs = get_unique_IDs(new_files) #\n",
    "    #print(trackIDs)\n",
    "    print(\"# tracks \", len(green_traces))\n",
    "    \n",
    "    _, green_w, red_w, _ = get_values(files, nframes, column_name=\"sigma (um)\")\n",
    "    print(\"# tracks \", len(green_w))\n",
    "    \n",
    "    green_traces = green_traces[mask]\n",
    "    red_traces = red_traces[mask]\n",
    "    reen_w = green_w[mask]\n",
    "    red_w = red_w[mask]\n",
    "    trackIDs = trackIDs[mask] ## !!! without this the trackIDs are incorrect\n",
    "    \n",
    "    #--- delay ----#\n",
    "    \n",
    "    try:\n",
    "        delay = date_cond2delay[(date, sample)]\n",
    "        print(delay)\n",
    "    except KeyError:\n",
    "        print(\"Key Error: \", date, sample)\n",
    "        delay = 20\n",
    "\n",
    "    #----------- Filter traces ----------#\n",
    "\n",
    "    final_green = []\n",
    "    final_red = []\n",
    "    final_IDs = []\n",
    "    \n",
    "    final_gw = []\n",
    "    final_rw = []\n",
    "    \n",
    "    for n in range(len(green_traces)):\n",
    "        \n",
    "        y = green_traces[n]\n",
    "        mask = ~np.isnan(y)\n",
    "\n",
    "        axs[0].plot(time/60., y, c='g', alpha=0.2)\n",
    "        axs[1].plot(time/60., red_traces[n], c='r', alpha=0.2)\n",
    "        \n",
    "        \n",
    "        if time[mask][0] + delay <= 100: # starting at most 100s after harringtonine # this if you are looking at harringtonine data\n",
    "                                         # > 0 (no filtering) for steady state data \n",
    "        \n",
    "            final_green.append(y)\n",
    "            final_red.append(red_traces[n])\n",
    "            final_IDs.append(trackIDs[n])\n",
    "            final_gw.append(green_w[n])\n",
    "            final_rw.append(red_w[n])\n",
    "\n",
    "    plt.xlim(-1, 20)\n",
    "    axs[0].set_ylim(-10, 300)\n",
    "    axs[1].set_ylim(-10, 300)\n",
    "   \n",
    "    plt.xlabel(\"time (min)\")\n",
    "    plt.ylabel(\"intensity (au)\")\n",
    "\n",
    "    final_green = np.array(final_green)\n",
    "    final_red = np.array(final_red)\n",
    "    final_IDs = np.array(final_IDs)\n",
    "    \n",
    "    final_gw = np.array(final_gw)\n",
    "    final_rw = np.array(final_rw)\n",
    "    print(final_IDs)\n",
    "    \n",
    "    ntraces = len(final_green)\n",
    "    \n",
    "    print(\"# filtered tracks \", ntraces, \"\\n\")\n",
    "    \n",
    "    #---- Save into dictionary ---- #\n",
    "    \n",
    "    date2green[date] = final_green\n",
    "    date2red[date] = final_red\n",
    "    date2IDs[date] = final_IDs\n",
    "    date2gw[date] = final_gw\n",
    "    date2rw[date] = final_rw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1100ed3",
   "metadata": {},
   "source": [
    "# Run-off starts after 60s from HT addition\n",
    "\n",
    "Here we save data as JSON file in a format that can be used as input for the Stan HMM code.\n",
    "Time t = 0 correspond to 60s after harringtonine addition.\n",
    "\n",
    "The red traces are also added to the JSON file even if not necessary to the HMM analysis (at present)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a919f985",
   "metadata": {},
   "outputs": [],
   "source": [
    "for nd, date in enumerate(list(sample2dates[sample])):\n",
    "    print(date)\n",
    "    \n",
    "    Yg = date2green[date]\n",
    "    Yr = date2red[date]\n",
    "    IDs = date2IDs[date]\n",
    "    \n",
    "    final_green = []\n",
    "    final_red = []\n",
    "    final_green_corr = []\n",
    "    final_ids = []\n",
    "  \n",
    "    for n in range(len(Yg)):\n",
    "            \n",
    "        yg = Yg[n]\n",
    "        yr = Yr[n]\n",
    "        mask = ~np.isnan(yg)\n",
    "            \n",
    "        green_corr = remove_spikes(yg)#remove_positive_corr(yg, yr)\n",
    "        \n",
    "        trackid = IDs[n]\n",
    "\n",
    "            \n",
    "        if np.all(green_corr[mask] >= 0):\n",
    "                \n",
    "            final_green_corr.append(green_corr) # corrected\n",
    "            final_red.append(yr)\n",
    "            final_green.append(yg)\n",
    "            final_ids.append(trackid)\n",
    "            \n",
    "        \n",
    "    final_green = np.array(final_green)\n",
    "    final_red = np.array(final_red)\n",
    "    final_green_corr = np.array(final_green_corr)\n",
    "    final_ids = np.array(final_ids, dtype=int)\n",
    "\n",
    "    ntraces = len(final_green)\n",
    "    \n",
    "        \n",
    "    #----- Remove NaNs and define the time ------#\n",
    "    \n",
    "    # Delay from harringtonine addition\n",
    "\n",
    "    try:\n",
    "        delay = date_cond2delay[(date, sample)]\n",
    "        print(delay)\n",
    "    except KeyError:\n",
    "        print(\"Key Error: \", date, sample)\n",
    "        delay = 20\n",
    "\n",
    "    times = np.array([time + delay] * ntraces) # TIME AFTER HT ADDITION\n",
    "\n",
    "    traces_nonan = []\n",
    "    traces_corr_nonan = []\n",
    "    times_nonan = []\n",
    "    reds_nonan = []\n",
    "\n",
    "    starts = [0]\n",
    "    for i in range(ntraces):\n",
    "        \n",
    "        # 1) we want to remove the intial and final point because they are subject to noise\n",
    "        # 2) we want also the trace to start not earlier than 60s after HT addition\n",
    "        \n",
    "        mask = ~np.isnan(final_green[i])\n",
    "        time_HT = times[i][mask][1 : -1] # trace time with respect to HT addition\n",
    "        y = final_green[i][mask][1 : -1]\n",
    "        ycorr = final_green_corr[i][mask][1 : -1]\n",
    "        yred = final_red[i][mask][1 : -1]\n",
    "        \n",
    "        if time_HT[0] < 60:\n",
    "            print(time_HT)\n",
    "            idx = np.arange(len(time_HT))\n",
    "            i_60 = idx[time_HT >= 60][0]\n",
    "            time_HT = time_HT[i_60 : ]\n",
    "            y = y[i_60 : ]\n",
    "            ycorr = ycorr[i_60 : ]\n",
    "            yred = yred[i_60 : ]\n",
    "\n",
    "        traces_nonan.append(y) \n",
    "        traces_corr_nonan.append(ycorr)\n",
    "        \n",
    "        times_nonan.append(time_HT - 60) # t = 0 correspond to run-off start not HT addition \n",
    "    \n",
    "        reds_nonan.append(yred)\n",
    "        if i < ntraces-1:\n",
    "            starts.append(starts[i] + len(traces_nonan[i]))\n",
    "        \n",
    "        plt.figure( figsize=(8,5))\n",
    "        plt.plot((time_HT - 60)/60, y)\n",
    "        plt.plot((time_HT - 60)/60, yred, c='r')\n",
    "        plt.title(final_ids[i])\n",
    "        plt.ylim(0, 200)\n",
    "\n",
    "    \n",
    "    \n",
    "        \n",
    "    ntraces = len(traces_nonan)\n",
    "    print(ntraces)\n",
    "    n_inactive = ntraces - estimate_n_translated(traces_nonan, thr = 10)\n",
    "    print(\" # inactive\", n_inactive)\n",
    "    \n",
    "    starts = [0]\n",
    "    for i in range(ntraces-1):\n",
    "        starts.append(starts[i] + len(traces_nonan[i]))\n",
    "    \n",
    "    #----- Concatenate traces ------#\n",
    "    \n",
    "    starts = np.array(starts)\n",
    "    \n",
    "    traces = np.concatenate(traces_nonan)\n",
    "    traces_corr = np.concatenate(traces_corr_nonan)\n",
    "    times = np.concatenate(times_nonan)\n",
    "\n",
    "    reds = np.concatenate(reds_nonan)\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    data = {\n",
    "        'ndata' : len(traces),\n",
    "        'ntraces' : ntraces,\n",
    "        'n_inactive' : n_inactive,\n",
    "        'I' : traces.tolist(),\n",
    "        'I_red' : reds.tolist(),\n",
    "        't' : times.tolist(),\n",
    "        'starts' : (starts + 1).tolist(),\n",
    "        'time_step' : dt,\n",
    "        'delay' : 0,\n",
    "        'trackIDs' : final_ids.tolist()\n",
    "        }\n",
    "    \n",
    "    #----- Save raw data as JSON file ------#\n",
    "    \n",
    "    json_file = join(output_dir, \"01HT_raw_{}_{}_ROstart60.json\".format(sample, date))\n",
    "    print(\"Saving Stan data as json file:\", json_file)   \n",
    "    with open(json_file, \"w\") as f:\n",
    "        json.dump(data, f)\n",
    "        \n",
    "    # --------------------------------- #\n",
    "    # corrected traces\n",
    "    \n",
    "    data = {\n",
    "        'ndata' : len(traces_corr),\n",
    "        'ntraces' : ntraces,\n",
    "        'n_inactive' : n_inactive,\n",
    "        'I' : traces_corr.tolist(),\n",
    "        'I_red' : reds.tolist(),\n",
    "        't' : times.tolist(),\n",
    "        'starts' : (starts + 1).tolist(),\n",
    "        'time_step' : dt,\n",
    "        'delay' : 0,\n",
    "        'trackIDs' : final_ids.tolist()\n",
    "        }\n",
    "    \n",
    "    #----- Save corrected data as JSON file ------#\n",
    "    \n",
    "    json_file = join(output_dir, \"01HT_corr_{}_{}_ROstart60.json\".format(sample, date))\n",
    "    print(\"Saving Stan data as json file:\", json_file)   \n",
    "    with open(json_file, \"w\") as f:\n",
    "        json.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c43273-aa07-4754-b8d7-b7cc0ce2fc89",
   "metadata": {},
   "source": [
    "## Save control and perturbed conditions in the same file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242e0e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save control and perturbed conditions in the same file\n",
    "\n",
    "reporter = \"COL\"\n",
    "sample=\"COL-H-AG*_01\"\n",
    "for date in list(sample2dates[sample]):\n",
    "    \n",
    "    print(date)\n",
    "    \n",
    "    files = []\n",
    "    files.append(\"01HT_corr_{}_{}_ROstart60.json\".format(reporter+\"-H-AG*_01\", date))\n",
    "    files.append(\"01HT_corr_{}_{}_ROstart60.json\".format(reporter+\"-H-GC7*_01\", date))\n",
    "    \n",
    "    # store in a list data per condition\n",
    "    ndata_l = []\n",
    "    ntraces_l = []\n",
    "    finactive_l = []\n",
    "    I_l = []\n",
    "    t_l = []\n",
    "    starts_l = []\n",
    "    trackIDs_l = []\n",
    "    \n",
    "    try:\n",
    "        for i, file in enumerate(files):\n",
    "\n",
    "            with open(join(output_dir, file)) as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            ndata_l.append(data[\"ndata\"])\n",
    "            ntraces_l.append(data[\"ntraces\"])\n",
    "            finactive_l.append(data[\"n_inactive\"] / data[\"ntraces\"])\n",
    "            I_l.append(data[\"I\"])\n",
    "            t_l.append(data[\"t\"])\n",
    "            trackIDs_l.append(data[\"trackIDs\"])\n",
    "            \n",
    "            if i > 0:\n",
    "                starts = [ndata_l[i - 1] + s for s in data[\"starts\"]]\n",
    "                starts_l.append(starts)\n",
    "            else:\n",
    "                starts_l.append(data[\"starts\"])\n",
    "                \n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n",
    "        continue\n",
    "        \n",
    "    \n",
    "    ndata = sum(ndata_l)\n",
    "    ntraces = sum(ntraces_l)\n",
    "    f_inactive = finactive_l\n",
    "    I = np.concatenate(I_l)\n",
    "    t = np.concatenate(t_l)\n",
    "    starts = np.concatenate(starts_l)\n",
    "    trackIDs = np.concatenate(trackIDs_l)\n",
    "    \n",
    "    cond_starts = [starts_l[0][0], starts_l[1][0]]\n",
    "    \n",
    "    data = {\n",
    "            'ncond' : 2,\n",
    "            'ndata' : ndata,\n",
    "            'ntraces' : ntraces,\n",
    "            'f_inactive' : f_inactive,\n",
    "            'I' : I.tolist(),\n",
    "            't' : t.tolist(),\n",
    "            'starts' : starts.tolist(),\n",
    "            'cond_starts' : cond_starts,\n",
    "            'time_step' : dt,\n",
    "            'trackIDs' : trackIDs.tolist()\n",
    "            }\n",
    "    \n",
    "    #----- Save data as JSON file ------#\n",
    "    \n",
    "    json_file = join(output_dir, \"01HT_corr_{}_{}_ROstart60.json\".format(reporter, date))\n",
    "    print(\"Saving Stan data as json file:\", json_file)   \n",
    "    with open(json_file, \"w\") as f:\n",
    "        json.dump(data, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
